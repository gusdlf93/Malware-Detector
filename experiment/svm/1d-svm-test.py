import os
import cv2

import time
import numpy as np
from Code.utils import *
from keras.models import *
from keras.layers import *

from keras import backend as K
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

from keras.utils import np_utils
from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard
from keras.models import Sequential, Model
from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, GlobalAveragePooling2D
from keras.optimizers import Adam
from keras.applications.resnet50 import ResNet50
#from keras_radam import RAdam
import glob
from Code.VGGUnet import  VGGUnet, VGGUnet2, VGGUnet3

optional_path = [r'/tmp/pycharm_project_803/Data/data_sec_20', r'/tmp/pycharm_project_803/Data/dex_20']

label_list = os.listdir(optional_path[1])
label = {label_list[i] : i for i in range(len(label_list))}

def recursive_find(path, path_list):
    # 재귀적으로 함수의 절대 경로를 출력합니다.
    # path는 탐색을 시작할 폴더, path_list는 return 해줄 절대 경로의 list입니다.
    files = os.listdir(path)

    for file in files:
        full_path = os.path.join(path, file)
        recursive_find(full_path, path_list) if os.path.isdir(full_path) else path_list.append(full_path)

    return path_list


def get_data(path):

    temp_img = cv2.imread(path, cv2.IMREAD_GRAYSCALE) / 255

    temp_img = np.asarray(temp_img)
    reshape_img = np.reshape(temp_img, (temp_img.shape[0] * temp_img.shape[1]))
    reshape_img = np.expand_dims(reshape_img, axis=1)

    return reshape_img

def imageSegmentationGenerator(X, Y, batch_size):

    zipped = itertools.cycle(zip(X, Y))

    while True:
        X = []
        Y = []
        for _ in range(batch_size):
            [i_img, i_label] = next(zipped)
            X.append(get_data(i_img))
            Y.append(i_label)
        yield np.array(X), np.array(Y)


def signal(n_class):
    img_input = Input(shape=(None, 1))

    x = Conv1D(64, kernel_size=(3),  activation='relu')(img_input)
    x = MaxPooling1D(pool_size=2)(x)
    x = Conv1D(128, (3), activation='relu')(x)
    x = MaxPooling1D(pool_size=2)(x)
    x = Conv1D(256, (3), activation='relu')(x)
    x = MaxPooling1D(pool_size=2)(x)
    x = Conv1D(512, (3), activation='relu')(x)
    x = GlobalAveragePooling1D()(x)
    x = Dropout(0.5)(x)
    x = Dense(512, activation='relu')(x)
    x = Dropout(0.5)(x)
    x = Dense(n_class, activation='softmax')(x)
    model = Model(img_input, x)

    return model

def load_model(path):
    img_input = Input(shape=(None, 1))

    x = Conv1D(64, kernel_size=(3),  activation='relu')(img_input)
    x = MaxPooling1D(pool_size=2)(x)
    x = Conv1D(128, (3), activation='relu')(x)
    x = MaxPooling1D(pool_size=2)(x)
    x = Conv1D(256, (3), activation='relu')(x)
    x = MaxPooling1D(pool_size=2)(x)
    x = Conv1D(512, (3), activation='relu')(x)
    _x = GlobalAveragePooling1D()(x)
    x = Dropout(0.5)(_x)
    x = Dense(512, activation='relu')(x)
    x = Dropout(0.5)(x)
    x = Dense(20, activation='softmax')(x)
    org_model = Model(img_input, x)
    org_model.load_weights(path)
    model =Model(img_input, _x)
    return model
def categorical_focal_loss(alpha, gamma=2.):
    """
    Softmax version of focal loss.
    When there is a skew between different categories/labels in your data set, you can try to apply this function as a
    loss.
           m
      FL = ∑  -alpha * (1 - p_o,c)^gamma * y_o,c * log(p_o,c)
          c=1
      where m = number of classes, c = class and o = observation
    Parameters:
      alpha -- the same as weighing factor in balanced cross entropy. Alpha is used to specify the weight of different
      categories/labels, the size of the array needs to be consistent with the number of classes.
      gamma -- focusing parameter for modulating factor (1-p)
    Default value:
      gamma -- 2.0 as mentioned in the paper
      alpha -- 0.25 as mentioned in the paper
    References:
        Official paper: https://arxiv.org/pdf/1708.02002.pdf
        https://www.tensorflow.org/api_docs/python/tf/keras/backend/categorical_crossentropy
    Usage:
     model.compile(loss=[categorical_focal_loss(alpha=[[.25, .25, .25]], gamma=2)], metrics=["accuracy"], optimizer=adam)
    """

    alpha = np.array(alpha, dtype=np.float32)

    def categorical_focal_loss_fixed(y_true, y_pred):
        """
        :param y_true: A tensor of the same shape as `y_pred`
        :param y_pred: A tensor resulting from a softmax
        :return: Output tensor.
        """

        # Clip the prediction value to prevent NaN's and Inf's
        epsilon = K.epsilon()
        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)

        # Calculate Cross Entropy
        cross_entropy = -y_true * K.log(y_pred)

        # Calculate Focal Loss
        loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy

        # Compute mean loss in mini_batch
        return K.mean(K.sum(loss, axis=-1))

    return categorical_focal_loss_fixed

scaling = 1
option = 1  # 0 == Using Section, 1 == Using Dex
model_option = 3 #0 = basic, 1 = resnet, 2 = efficientnet, 3 = unet

batch_size = 1
epoch = 1000
val = 0.15
n_class = 20

X = []
X = recursive_find(optional_path[option], X)
Y = [label[X[i].split('/')[-2]] for i in range(len(X))]
Y = np_utils.to_categorical(Y)

X_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, stratify=Y, test_size=0.2, random_state=42)
X_Train, X_Val, Y_Train, Y_Val = train_test_split(X_Train, Y_Train, stratify=Y_Train, test_size=0.15, random_state=42)

Train = imageSegmentationGenerator(X_Train, Y_Train, batch_size)
Val = imageSegmentationGenerator(X_Val, Y_Val, batch_size)
Test = imageSegmentationGenerator(X_Test, Y_Test, batch_size)

l_Train = len(Train.gi_frame.f_locals['X'])
l_Val = len(Val.gi_frame.f_locals['X'])
l_Test = len(Test.gi_frame.f_locals['X'])

model = signal(n_class)

model.summary()

model_path = '/tmp/pycharm_project_803/Code/10-11_dex_20signal(l=0.39,acc=93).h5'
from sklearn import svm
model = load_model(model_path)
X_Train = model.predict_generator(Train, steps=l_Train).reshape(l_Train, 512)
X_Test = model.predict_generator(Test, steps=l_Test).reshape(l_Test, 512)

clf = svm.SVC(kernel='linear')

from sklearn.datasets import make_blobs

clf.fit(X_Train, np.argmax(Y_Train, axis=1))

Y_Pred_class = clf.predict(X_Test)
Y_Test_class = np.argmax(Y_Test, axis=1)

print(classification_report(Y_Test_class, Y_Pred_class, digits=3))
print(confusion_matrix(Y_Test_class, Y_Pred_class))
