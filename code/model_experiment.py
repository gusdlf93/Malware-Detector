import cv2
import os, glob, numpy as np
from sklearn.model_selection import train_test_split
import cv2
from keras.utils import np_utils
from PIL import Image
import PIL
from keras.applications.inception_v3 import InceptionV3
from keras_radam import RAdam

import tensorflow as tf
config = tf.ConfigProto()
config.gpu_options.allow_growth=True
session = tf.Session(config=config)

def recursive_find(path, path_list):
    # 재귀적으로 함수의 절대 경로를 출력합니다.
    # path는 탐색을 시작할 폴더, path_list는 return 해줄 절대 경로의 list입니다.
    files = os.listdir(path)

    for file in files:
        full_path = os.path.join(path, file)
        recursive_find(full_path, path_list) if os.path.isdir(full_path) else path_list.append(full_path)

    return path_list


def get_data(path):
    img = []
    label = []
    for i_path in path:
        abs_path = i_path.split('\\')

        temp_img = Image.open(i_path)
        #temp_img = temp_img.convert("RGB")
        temp_img = temp_img.convert("L")
        temp_img = temp_img.resize((image_w, image_h), resample=PIL.Image.BICUBIC)

        img.append(np.array(temp_img))
        label.append(abs_path[-2])

    label = [target[i] for i in label]
    label = np.asarray(label)
    label = np_utils.to_categorical(label)
    return img, label


def make_data(path):
    path_list = []
    data_path_list = recursive_find(path, path_list)
    return get_data(data_path_list)


image_w, image_h, color = 380, 380, 1
target = dict([[name, _] for _, name in enumerate(os.listdir(r'D:\android\Data\dex_20'))])

X, Y = make_data(r'D:\android\Data\dex_20')

X = np.array(X)
y = np.array(Y)


X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y,test_size=0.2,random_state = 0)
xy = (X_train, X_test, y_train, y_test)

print("ok", len(y))

X_train = X_train.reshape(X_train.shape[0], image_h, image_w, color).astype('float32')# / 255
X_test = X_test.reshape(X_test.shape[0], image_h, image_w, color).astype('float32')# / 255

print(X_train.shape)
print(X_train.shape[0])
print(y_train.shape)

import os, glob, numpy as np
from keras.models import Sequential
from keras.layers import Conv2D,Conv3D, MaxPooling2D, Dense, Flatten, Dropout
from keras.callbacks import EarlyStopping, ModelCheckpoint
import matplotlib.pyplot as plt
import keras.backend.tensorflow_backend as K
from keras.optimizers import SGD, RMSprop

import tensorflow as tf
from keras import optimizers
from keras.utils import np_utils
from keras.applications.inception_v3 import InceptionV3
from keras.applications.resnet50 import ResNet50
import sys
from keras.layers import GlobalAveragePooling2D
from keras.models import Sequential, Model
import efficientnet.keras as efn

base_model = efn.EfficientNetB0(weights=None, include_top=False)
#base_model = ResNet50(weights=None, include_top=False)
#base_model = InceptionV3(weights=None, include_top=False)

from efficientnet import keras as efficient_keras
from keras.layers import BatchNormalization

def efficient_model(n_class):
    base_model = efficient_keras.EfficientNetB4(weights = None, include_top=False, input_shape=(image_h, image_w, 1))
    for layer in base_model.layers:
        layer.trainable = True

    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    x = BatchNormalization()(x)
    x = Dense(1024, activation='relu')(x)
    predict = Dense(n_class, activation='softmax')(x)
    model = Model(inputs=base_model.input, output=predict)
    return model

# for layer in base_model.layers:
#     layer.trainable = True
# x = base_model.output
# x = GlobalAveragePooling2D()(x)
#x = Dense(512, activation='relu')(x)
# x = Dropout(0.3)(x)
# predict = Dense(20, activation='softmax')(x)
# model = Model(inputs=base_model.input, output=predict)

model = efficient_model(20)
sgd = optimizers.SGD(lr=0.01, momentum = 0.9, decay=1e-5)
model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])
#model.compile(loss='categorical_crossentropy', optimizer=RAdam(), metrics=['accuracy'])

modelName = r"F:\PycharmProjects\Android_pytorch\EfficientNet0.h5"
checkpointer = ModelCheckpoint(filepath=modelName, monitor='val_loss', verbose=1, save_best_only = True)
early_stopping = EarlyStopping(monitor='val_loss', patience=20)

model.summary()

import time
history = model.fit(X_train, y_train, epochs=100, batch_size=2,validation_split= 0.15, callbacks=[early_stopping, checkpointer])

import numpy as np
from sklearn.metrics import classification_report,confusion_matrix
model.load_weights(modelName)
y_pred = model.predict(X_test)
y_test_class = np.argmax(y_test,axis=1)
y_pred_class = np.argmax(y_pred,axis=1)
print(classification_report(y_test_class,y_pred_class))